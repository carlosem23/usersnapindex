{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_index(directory_path):\n",
    "  # set maximum input size\n",
    "  max_input_size = 4096\n",
    "  # set number of output tokens\n",
    "  num_outputs = 256\n",
    "  # set maximum chunk overlap\n",
    "  max_chunk_overlap = 20\n",
    "  # set chunk size limit\n",
    "  chunk_size_limit = 600\n",
    "\n",
    "  prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "  # define LLM\n",
    "  llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    "  \n",
    "  documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "  \n",
    "  index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "  \n",
    "  index.save_to_disk('index.json')\n",
    "  \n",
    "  return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bot(input_index = 'index.json'):\n",
    "  index = GPTSimpleVectorIndex.load_from_disk(input_index)\n",
    "  while True:\n",
    "    query = input('What do you want to ask the bot?   \\n')\n",
    "    response = index.query(query, response_mode=\"compact\")\n",
    "    print (\"\\nBot says: \\n\\n\" + response.response + \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 5137 tokens\n"
     ]
    }
   ],
   "source": [
    "index = construct_index(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'include_prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ask_bot(\u001b[39m'\u001b[39;49m\u001b[39mindex.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mask_bot\u001b[1;34m(input_index)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m   query \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mWhat do you want to ask the bot?   \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m   response \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mquery(query, response_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcompact\u001b[39;49m\u001b[39m\"\u001b[39;49m, include_prompt\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      6\u001b[0m   \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mBot says: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m response\u001b[39m.\u001b[39mresponse \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mahdi\\anaconda3\\envs\\userindex\\lib\\site-packages\\gpt_index\\indices\\base.py:417\u001b[0m, in \u001b[0;36mBaseGPTIndex.query\u001b[1;34m(self, query_str, mode, query_transform, use_async, **query_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m query_config \u001b[39m=\u001b[39m QueryConfig(\n\u001b[0;32m    402\u001b[0m     index_struct_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct\u001b[39m.\u001b[39mget_type(),\n\u001b[0;32m    403\u001b[0m     query_mode\u001b[39m=\u001b[39mmode_enum,\n\u001b[0;32m    404\u001b[0m     query_kwargs\u001b[39m=\u001b[39mquery_kwargs,\n\u001b[0;32m    405\u001b[0m )\n\u001b[0;32m    406\u001b[0m query_runner \u001b[39m=\u001b[39m QueryRunner(\n\u001b[0;32m    407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm_predictor,\n\u001b[0;32m    408\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prompt_helper,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     use_async\u001b[39m=\u001b[39muse_async,\n\u001b[0;32m    416\u001b[0m )\n\u001b[1;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m query_runner\u001b[39m.\u001b[39;49mquery(query_str, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_struct)\n",
      "File \u001b[1;32mc:\\Users\\mahdi\\anaconda3\\envs\\userindex\\lib\\site-packages\\gpt_index\\indices\\query\\query_runner.py:126\u001b[0m, in \u001b[0;36mQueryRunner.query\u001b[1;34m(self, query_str_or_bundle, index_struct)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     query_bundle \u001b[39m=\u001b[39m query_str_or_bundle\n\u001b[1;32m--> 126\u001b[0m query_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_query_obj(index_struct)\n\u001b[0;32m    127\u001b[0m \u001b[39mreturn\u001b[39;00m query_obj\u001b[39m.\u001b[39mquery(query_bundle)\n",
      "File \u001b[1;32mc:\\Users\\mahdi\\anaconda3\\envs\\userindex\\lib\\site-packages\\gpt_index\\indices\\query\\query_runner.py:103\u001b[0m, in \u001b[0;36mQueryRunner._get_query_obj\u001b[1;34m(self, index_struct)\u001b[0m\n\u001b[0;32m    101\u001b[0m query_runner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m    102\u001b[0m query_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_query_kwargs(config)\n\u001b[1;32m--> 103\u001b[0m query_obj \u001b[39m=\u001b[39m query_cls(\n\u001b[0;32m    104\u001b[0m     index_struct,\n\u001b[0;32m    105\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mquery_kwargs,\n\u001b[0;32m    106\u001b[0m     query_runner\u001b[39m=\u001b[39mquery_runner,\n\u001b[0;32m    107\u001b[0m     docstore\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docstore,\n\u001b[0;32m    108\u001b[0m     recursive\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recursive,\n\u001b[0;32m    109\u001b[0m     use_async\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async,\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m query_obj\n",
      "File \u001b[1;32mc:\\Users\\mahdi\\anaconda3\\envs\\userindex\\lib\\site-packages\\gpt_index\\indices\\query\\vector_store\\queries.py:64\u001b[0m, in \u001b[0;36mGPTSimpleVectorIndexQuery.__init__\u001b[1;34m(self, index_struct, simple_vector_store_data_dict, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     vector_store \u001b[39m=\u001b[39m SimpleVectorStore(\n\u001b[0;32m     62\u001b[0m         simple_vector_store_data_dict\u001b[39m=\u001b[39msimple_vector_store_data_dict\n\u001b[0;32m     63\u001b[0m     )\n\u001b[1;32m---> 64\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(index_struct\u001b[39m=\u001b[39mindex_struct, vector_store\u001b[39m=\u001b[39mvector_store, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mahdi\\anaconda3\\envs\\userindex\\lib\\site-packages\\gpt_index\\indices\\query\\vector_store\\base.py:34\u001b[0m, in \u001b[0;36mGPTVectorStoreIndexQuery.__init__\u001b[1;34m(self, index_struct, vector_store, embed_model, similarity_top_k, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     26\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     27\u001b[0m     index_struct: IndexDict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m     32\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(index_struct\u001b[39m=\u001b[39mindex_struct, embed_model\u001b[39m=\u001b[39membed_model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_similarity_top_k \u001b[39m=\u001b[39m similarity_top_k\n\u001b[0;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m vector_store \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'include_prompt'"
     ]
    }
   ],
   "source": [
    "ask_bot('index.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "userindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
